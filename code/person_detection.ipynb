{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load the COCO class labels\n",
    "CLASSES = [\n",
    "    'person', 'bicycle'\n",
    "]\n",
    "\n",
    "# Define the function to perform human detection on a video\n",
    "def detect_human(video_path, output_dir):\n",
    "    video_name = os.path.basename(video_path)\n",
    "    output_path = os.path.join(output_dir, video_name)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image_tensor = torchvision.transforms.functional.to_tensor(frame)\n",
    "        outputs = model([image_tensor])\n",
    "\n",
    "        # Get the bounding boxes and labels for human detections\n",
    "        boxes = outputs[0]['boxes']\n",
    "        labels = outputs[0]['labels']\n",
    "\n",
    "        # Filter out detections where the label corresponds to 'person'\n",
    "        human_boxes = [boxes[i] for i in range(len(boxes)) if labels[i] == 1]\n",
    "\n",
    "        # Draw bounding boxes around detected humans\n",
    "        for box in human_boxes:\n",
    "            box = box.detach().numpy().astype(int)\n",
    "            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with bounding boxes to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "input_dir = r'E:\\Drive D\\MA-ICT Convergence\\Thesis\\Human-Human-Interaction\\dataset\\Ch4'\n",
    "output_dir = r'E:\\Drive D\\MA-ICT Convergence\\Thesis\\Human-Human-Interaction\\dataset\\ch4_output_person_detection'\n",
    "\n",
    "# Iterate through all video files in the input directory\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.mp4'):\n",
    "            video_path = os.path.join(root, file)\n",
    "            detect_human(video_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\user\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: torch==2.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (2.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from torch==2.2.1->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.2.1->torchvision) (2022.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch==2.2.1->torchvision) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy->torch==2.2.1->torchvision) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2022.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall numpy==1.19.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import onnx\n",
    "from onnx import shape_inference\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define paths for saving the model\n",
    "model_save_dir = r'E:\\Drive D\\MA-ICT Convergence\\Semester 4\\Human-Human-Interaction\\src\\model'\n",
    "frozen_graph_path = os.path.join(model_save_dir, 'frozen_inference_graph.pb')\n",
    "graph_txt_path = os.path.join(model_save_dir, 'graph.pbtxt')\n",
    "\n",
    "# Save the model as a pre-trained Frozen Graph\n",
    "input_names = ['image']\n",
    "output_names = ['boxes', 'labels', 'scores']\n",
    "torch.onnx.export(model, torch.rand(1, 3, 300, 300), frozen_graph_path, input_names=input_names, output_names=output_names, opset_version=11)\n",
    "\n",
    "# Generate a graph text file for visualization\n",
    "onnx_model = onnx.load(frozen_graph_path)\n",
    "onnx_model = shape_inference.infer_shapes(onnx_model)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "with open(graph_txt_path, 'w') as f:\n",
    "    f.write(str(onnx_model.graph))\n",
    "\n",
    "# Define the function to perform human detection on a video\n",
    "def detect_human(video_path, output_dir):\n",
    "    video_name = os.path.basename(video_path)\n",
    "    output_path = os.path.join(output_dir, video_name)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (frame_width, frame_height))\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image_tensor = torchvision.transforms.functional.to_tensor(frame)\n",
    "        outputs = model([image_tensor])\n",
    "\n",
    "        # Get the bounding boxes and labels for human detections\n",
    "        boxes = outputs[0]['boxes']\n",
    "        labels = outputs[0]['labels']\n",
    "\n",
    "        # Filter out detections where the label corresponds to 'person'\n",
    "        human_boxes = [boxes[i] for i in range(len(boxes)) if labels[i] == 1]\n",
    "\n",
    "        # Draw bounding boxes around detected humans\n",
    "        for box in human_boxes:\n",
    "            box = box.detach().cpu().numpy().astype(int)\n",
    "            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with bounding boxes to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Define the paths for input and output directories\n",
    "input_dir = r'E:\\Drive D\\MA-ICT Convergence\\Semester 4\\Human-Human-Interaction\\dataset\\train'\n",
    "output_dir = r'E:\\Drive D\\MA-ICT Convergence\\Semester 4\\Human-Human-Interaction\\dataset\\output_person_detection'\n",
    "\n",
    "# Iterate through all video files in the input directory\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.avi'):\n",
    "            video_path = os.path.join(root, file)\n",
    "            detect_human(video_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import onnx\n",
    "from onnx import shape_inference\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define paths for saving the model\n",
    "model_save_dir = r'E:\\Drive D\\MA-ICT Convergence\\Semester 4\\Human-Human-Interaction\\src\\model'\n",
    "frozen_graph_path = os.path.join(model_save_dir, 'frozen_inference_graph.pb')\n",
    "graph_txt_path = os.path.join(model_save_dir, 'graph.pbtxt')\n",
    "\n",
    "# Save the model as a pre-trained Frozen Graph\n",
    "input_names = ['image']\n",
    "output_names = ['boxes', 'labels', 'scores']\n",
    "torch.onnx.export(model, torch.rand(1, 3, 300, 300), frozen_graph_path, input_names=input_names, output_names=output_names, opset_version=11)\n",
    "\n",
    "# Generate a graph text file for visualization\n",
    "onnx_model = onnx.load(frozen_graph_path)\n",
    "onnx_model = shape_inference.infer_shapes(onnx_model)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "with open(graph_txt_path, 'w') as f:\n",
    "    f.write(str(onnx_model.graph))\n",
    "\n",
    "# Define the function to perform human detection on a video\n",
    "def detect_human(video_path, output_dir):\n",
    "    video_name = os.path.basename(video_path)\n",
    "    output_path = os.path.join(output_dir, video_name)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (frame_width, frame_height))\n",
    "    \n",
    "    colors = [(0, 255, 0), (0, 0, 255)]  # Green, Red\n",
    "    color_index = 0\n",
    "    \n",
    "    # Dictionary to store color assigned to each detected person\n",
    "    person_colors = {}\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image_tensor = torchvision.transforms.functional.to_tensor(frame)\n",
    "        outputs = model([image_tensor])\n",
    "\n",
    "        # Get the bounding boxes and labels for human detections\n",
    "        boxes = outputs[0]['boxes']\n",
    "        labels = outputs[0]['labels']\n",
    "\n",
    "        # Filter out detections where the label corresponds to 'person'\n",
    "        human_boxes = [boxes[i] for i in range(len(boxes)) if labels[i] == 1]\n",
    "\n",
    "        # Draw bounding boxes around detected humans with different colors\n",
    "        for box in human_boxes:\n",
    "            box = box.detach().cpu().numpy().astype(int)\n",
    "            # Check if the person is already assigned a color\n",
    "            if tuple(box) in person_colors:\n",
    "                color = person_colors[tuple(box)]\n",
    "            else:\n",
    "                color = colors[color_index % len(colors)]  # Get color from list cyclically\n",
    "                person_colors[tuple(box)] = color  # Store color for this person\n",
    "                color_index += 1  # Increment color index for the next person\n",
    "            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "\n",
    "        # Write the frame with bounding boxes to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Define the paths for input and output directories\n",
    "input_dir = r'E:\\Drive D\\MA-ICT Convergence\\Semester 4\\Human-Human-Interaction\\dataset\\train_3'\n",
    "output_dir = r'E:\\Drive D\\MA-ICT Convergence\\Thesis\\Human-Human-Interaction\\dataset\\crc_output_person_tracking'\n",
    "\n",
    "# Iterate through all video files in the input directory\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.mp4'):\n",
    "            video_path = os.path.join(root, file)\n",
    "            detect_human(video_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add one more class: None Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3982: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  (torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\ops\\boxes.py:157: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\ops\\boxes.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\__init__.py:1499: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\detection\\transform.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\detection\\transform.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:5859: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import onnx\n",
    "from onnx import shape_inference\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define paths for saving the model\n",
    "model_save_dir = r'E:\\Drive D\\MA-ICT Convergence\\Semester 4\\Human-Human-Interaction\\src\\model'\n",
    "frozen_graph_path = os.path.join(model_save_dir, 'frozen_inference_graph.pb')\n",
    "graph_txt_path = os.path.join(model_save_dir, 'graph.pbtxt')\n",
    "\n",
    "# Save the model as a pre-trained Frozen Graph\n",
    "input_names = ['image']\n",
    "output_names = ['boxes', 'labels', 'scores']\n",
    "torch.onnx.export(model, torch.rand(1, 3, 300, 300), frozen_graph_path, input_names=input_names, output_names=output_names, opset_version=11)\n",
    "\n",
    "# Generate a graph text file for visualization\n",
    "onnx_model = onnx.load(frozen_graph_path)\n",
    "onnx_model = shape_inference.infer_shapes(onnx_model)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "with open(graph_txt_path, 'w') as f:\n",
    "    f.write(str(onnx_model.graph))\n",
    "\n",
    "# Define the function to perform human detection on a video\n",
    "def detect_human(video_path, output_dir):\n",
    "    video_name = os.path.basename(video_path)\n",
    "    output_path = os.path.join(output_dir, video_name)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (frame_width, frame_height))\n",
    "    \n",
    "    colors = [(0, 255, 0), (0, 0, 255)]  # Green, Red\n",
    "    color_index = 0\n",
    "    \n",
    "    # Dictionary to store color assigned to each detected person\n",
    "    person_colors = {}\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image_tensor = torchvision.transforms.functional.to_tensor(frame)\n",
    "        outputs = model([image_tensor])\n",
    "\n",
    "        # Get the bounding boxes and labels for human detections\n",
    "        boxes = outputs[0]['boxes']\n",
    "        labels = outputs[0]['labels']\n",
    "\n",
    "        # Filter out detections where the label corresponds to 'person'\n",
    "        human_boxes = [boxes[i] for i in range(len(boxes)) if labels[i] == 1]\n",
    "\n",
    "        # Draw bounding boxes around detected humans with different colors\n",
    "        for box in human_boxes:\n",
    "            box = box.detach().cpu().numpy().astype(int)\n",
    "            # Check if the person is already assigned a color\n",
    "            if tuple(box) in person_colors:\n",
    "                color = person_colors[tuple(box)]\n",
    "            else:\n",
    "                color = colors[color_index % len(colors)]  # Get color from list cyclically\n",
    "                person_colors[tuple(box)] = color  # Store color for this person\n",
    "                color_index += 1  # Increment color index for the next person\n",
    "            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "\n",
    "        # Write the frame with bounding boxes to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Define the paths for input and output directories\n",
    "input_dir = r'E:\\Drive D\\MA-ICT Convergence\\Thesis\\Human-Human-Interaction\\dataset\\none_interaction'\n",
    "output_dir = r'E:\\Drive D\\MA-ICT Convergence\\Thesis\\Human-Human-Interaction\\dataset\\none_interaction_output_person_frame'\n",
    "\n",
    "# Iterate through all video files in the input directory\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.mp4'):\n",
    "            video_path = os.path.join(root, file)\n",
    "            detect_human(video_path, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "cdae5de5f4d8d2cf607b638c9321211b47b4e6e6dce50d010e1b4cf259f5807b"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
