{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from moviepy.editor import *\n",
    "# from tensorflow.keras.layers import *\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from keras.models import load_model\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT, IMAGE_WIDTH = 128, 128\n",
    "SEQUENCE_LENGTH = 30\n",
    "CLASSES_LIST = ['hugging', 'pushing', 'pointing', 'none-interaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {\n",
    "    'Orthogonal': tf.keras.initializers.Orthogonal()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(r\"E:\\Drive D\\MA-ICT Convergence\\Thesis\\Human-Human-Interaction\\src\\model\\gru-v1.h5\", custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):\n",
    "    '''\n",
    "    This function will perform action recognition on a video using the LRCN model.\n",
    "    Args:\n",
    "    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.\n",
    "    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.\n",
    "    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.\n",
    "    '''\n",
    " \n",
    "    # Initialize the VideoCapture object to read from the video file.\n",
    "    video_reader = cv2.VideoCapture(video_file_path)\n",
    " \n",
    "    # Get the width and height of the video.\n",
    "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    " \n",
    "    # Initialize the VideoWriter Object to store the output video in the disk.\n",
    "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), \n",
    "                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n",
    " \n",
    "    # Declare a queue to store video frames.\n",
    "    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n",
    " \n",
    "    # Initialize a variable to store the predicted action being performed in the video.\n",
    "    predicted_class_name = ''\n",
    " \n",
    "    # Iterate until the video is accessed successfully.\n",
    "    while video_reader.isOpened():\n",
    " \n",
    "        # Read the frame.\n",
    "        ok, frame = video_reader.read() \n",
    "        \n",
    "        # Check if frame is not read properly then break the loop.\n",
    "        if not ok:\n",
    "            break\n",
    " \n",
    "        # Resize the Frame to fixed Dimensions.\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        \n",
    "        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.\n",
    "        normalized_frame = resized_frame / 255\n",
    " \n",
    "        # Appending the pre-processed frame into the frames list.\n",
    "        frames_queue.append(normalized_frame)\n",
    " \n",
    "        # Check if the number of frames in the queue are equal to the fixed sequence length.\n",
    "        if len(frames_queue) == SEQUENCE_LENGTH:\n",
    " \n",
    "            # Pass the normalized frames to the model and get the predicted probabilities.\n",
    "            predicted_labels_probabilities = model.predict(np.expand_dims(frames_queue, axis = 0))[0]\n",
    " \n",
    "            # Get the index of class with highest probability.\n",
    "            predicted_label = np.argmax(predicted_labels_probabilities)\n",
    " \n",
    "            # Get the class name using the retrieved index.\n",
    "            predicted_class_name = CLASSES_LIST[predicted_label]\n",
    " \n",
    "        # Write predicted class name on top of the frame.\n",
    "        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    " \n",
    "        # Write The frame into the disk using the VideoWriter Object.\n",
    "        video_writer.write(frame)\n",
    "        \n",
    "    # Release the VideoCapture and VideoWriter objects.\n",
    "    video_reader.release()\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_file_path = r\"E:\\Drive D\\MA-ICT Convergence\\Thesis\\Human-Human-Interaction\\dataset\\all_channels\\hugging\\c1\\hugging_c1_18.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(1, 30, 128, 128, 3), dtype=float32). Expected shape (None, 51, 100), but input has incompatible shape (1, 30, 128, 128, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 30, 128, 128, 3), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m output_video_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDrive D\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMA-ICT Convergence\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mThesis\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHuman-Human-Interaction\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124masset\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest-model-output\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124masset\u001b[39m\u001b[38;5;132;01m{SEQUENCE_LENGTH}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Perform Action Recognition on the Test Video.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mpredict_on_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_video_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_video_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEQUENCE_LENGTH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Display the output video.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m VideoFileClip(output_video_file_path, audio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, target_resolution\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m600\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m))\u001b[38;5;241m.\u001b[39mipython_display()\n",
      "Cell \u001b[1;32mIn[19], line 50\u001b[0m, in \u001b[0;36mpredict_on_video\u001b[1;34m(video_file_path, output_file_path, SEQUENCE_LENGTH)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Check if the number of frames in the queue are equal to the fixed sequence length.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frames_queue) \u001b[38;5;241m==\u001b[39m SEQUENCE_LENGTH:\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Pass the normalized frames to the model and get the predicted probabilities.\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     predicted_labels_probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Get the index of class with highest probability.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     predicted_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predicted_labels_probabilities)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\models\\functional.py:280\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    278\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    279\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(1, 30, 128, 128, 3), dtype=float32). Expected shape (None, 51, 100), but input has incompatible shape (1, 30, 128, 128, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 30, 128, 128, 3), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# Construct the output video path.\n",
    "output_video_file_path = r'E:\\Drive D\\MA-ICT Convergence\\Thesis\\Human-Human-Interaction\\asset\\test-model-output\\asset{SEQUENCE_LENGTH}.mp4'\n",
    "\n",
    "# Perform Action Recognition on the Test Video.\n",
    "predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)\n",
    "\n",
    "# Display the output video.\n",
    "VideoFileClip(output_video_file_path, audio=False, target_resolution=(600,None)).ipython_display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
